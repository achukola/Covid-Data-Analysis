{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3DESN5O75ooy",
    "outputId": "1cb5f592-16fd-42ec-8b25-90f4fe797596"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the given airline dataset\n",
    "pathairline = \"Tweets.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_of_tweets = pd.read_csv(pathairline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_of_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jUkb4zgC9S3",
    "outputId": "bf8d26f1-86c1-4be2-c60e-9c3d9ab275b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_of_tweets['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5FwawPRANsA"
   },
   "source": [
    "## Vectorizer and Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cklcEo3dBijJ",
    "outputId": "0c7cdd05-0bc3-4afa-bbd8-c608a94a836d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     @VirginAmerica it's really aggressive to blast...\n",
       "4     @VirginAmerica and it's a really big bad thing...\n",
       "5     @VirginAmerica seriously would pay $30 a fligh...\n",
       "15        @VirginAmerica SFO-PDX schedule is still MIA.\n",
       "17    @VirginAmerica  I flew from NYC to SFO last we...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_word_doc = df_of_tweets['text'].loc[df_of_tweets['airline_sentiment']=='negative']\n",
    "negative_word_doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOXVT9x9Cjq9",
    "outputId": "12fe1f1c-5b7a-4a25-e55d-f5844851d1d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     @VirginAmerica plus you've added commercials t...\n",
       "6     @VirginAmerica yes, nearly every time I fly VX...\n",
       "8       @virginamerica Well, I didn't…but NOW I DO! :-D\n",
       "9     @VirginAmerica it was amazing, and arrived an ...\n",
       "11    @VirginAmerica I &lt;3 pretty graphics. so muc...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_word_doc = df_of_tweets['text'].loc[df_of_tweets['airline_sentiment']=='positive']\n",
    "positive_word_doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords and punctuations from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopwords = []\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.extend(new_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to remove numbers\n",
    "expression = re.compile(r'\\d')\n",
    "negative_word_doc = pd.Series([re.sub(expression, '', word) for word in negative_word_doc])\n",
    "positive_word_doc = pd.Series([re.sub(expression, '', word) for word in positive_word_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gndDgyHjCqoF",
    "outputId": "1cd786f6-9b55-43c3-bb98-8ef5909aa472"
   },
   "outputs": [],
   "source": [
    "# remove stopwords url and @mentions from tweets\n",
    "negative_word_doc = negative_word_doc.apply(lambda x: ' '.join([word.lower() for word in x.split() if word.find('@') == -1 and \\\n",
    "                                                                word not in stopwords and \\\n",
    "                                                                word.find('http') == -1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_word_doc = positive_word_doc.apply(lambda x: ' '.join([word.lower() for word in x.split() if word.find('@') == -1 and \\\n",
    "                                                                word not in stopwords and \\\n",
    "                                                                word.find('http') == -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    really aggressive blast obnoxious \"entertainme...\n",
       "1                                 really big bad thing\n",
       "2    seriously would pay $ flight seats playing. re...\n",
       "3                          sfo-pdx schedule still mia.\n",
       "4    flew nyc sfo last week fully sit seat due two ...\n",
       "5    first fares may three times carriers seats ava...\n",
       "6    guys messed seating.. reserved seating friends...\n",
       "dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_word_doc[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'great', 'i', 'much', 'good', 'best', 'awesome', 'amazing', 'back', 'flying', 'fly', 'made', 'like', 'home', 'new', 'plane', 'first', 'one', 'well', 'nice', 'u', 'still', 'helpful', 'go', 'cancelled', 'better', 'next', 'southwest', 'last', 'sent', 'happy', 'weather', 'free', 'quick', 'done', 'attendant', 'even', 'united', 'right', 'able', 'sure', 'big', 'another', 'looking', 'up', 'forward', 'ok', 'excellent', 'wonderful', 'cool', 'glad'\n"
     ]
    }
   ],
   "source": [
    "#Top 50 adjectives in positive tweets\n",
    "adj_words = []\n",
    "for doc in positive_word_doc:\n",
    "    for word in word_tokenize(doc):\n",
    "        pos = wn.synsets(word, pos='a')\n",
    "        if len(pos) > 0:\n",
    "            adj_words.append(word)\n",
    "\n",
    "pos_adj_words = pd.Series(adj_words).value_counts()[:50]\n",
    "\n",
    "# print all words in positive tweets separated by comma and in quotes\n",
    "print(', '.join([\"'{}'\".format(word) for word in pos_adj_words.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'i', 'cancelled', 'plane', 'delayed', 'still', 'one', 'late', 'back', 'waiting', 'like', 'trying', 'going', 'even', 'u', 'worst', 'fly', 'last', 'lost', 'another', 'weather', 'due', 'united', 'flighted', 'go', 'home', 'sitting', 'two', 'flying', 'first', 'bad', 'stuck', 'online', 'said', 'next', 'good', 'new', 'right', 'made', 'better', 'rude', 'long', 'tried', 'missed', 'left', 'booked', 'sure', 'ago', 'on', 'terrible', 'sent'\n"
     ]
    }
   ],
   "source": [
    "#Top 50 adjectives in negative tweets\n",
    "adj_words = []\n",
    "for doc in negative_word_doc:\n",
    "    for word in word_tokenize(doc):\n",
    "        pos = wn.synsets(word, pos='a')\n",
    "        if len(pos) > 0:\n",
    "            adj_words.append(word)\n",
    "\n",
    "neg_adj_words = pd.Series(adj_words).value_counts()[:50]\n",
    "\n",
    "# print all words in negative tweets separated by comma and in quotes\n",
    "print(', '.join([\"'{}'\".format(word) for word in neg_adj_words.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "\tdocument_words = set(document)\n",
    "\tfeatures = {}\n",
    "\tfor word in word_features:\n",
    "\t\tfeatures['V_%s' % word] = (word in document_words)\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative tweets\n",
    "pos_tweets = [(document, 'positive') for document in positive_word_doc]\n",
    "neg_tweets = [(document, 'negative') for document in negative_word_doc]\n",
    "all_tweets = pos_tweets + neg_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('broken luggage mild case food poisoning ua lhr-iah. email this?',\n",
       "  'negative'),\n",
       " ('guys please give update? sitting tarmac flight yet website still says time?',\n",
       "  'negative'),\n",
       " ('flying las vegas chicago flight late flight announcement made', 'negative'),\n",
       " ('view downtown los angeles, hollywood sign, beyond rain mountains!',\n",
       "  'positive'),\n",
       " ('really ashamed. entire business class cabin empty almost first class...',\n",
       "  'negative'),\n",
       " ('- thank you!', 'positive'),\n",
       " ('charged one way baggage.find others leaving pittsburgh equipment charged hung me!!',\n",
       "  'negative'),\n",
       " ('wife &amp; infant daughter aa abi-dfw. en route delayed. y’all hold aa rdu connect?',\n",
       "  'negative'),\n",
       " ('yet staff philadelphia failed send luggage home me. #alwayshappensthere #angrytraveler',\n",
       "  'negative'),\n",
       " ('prepared weather. want find take train phl catch connection.', 'negative')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomly shuffling the tweets\n",
    "random.shuffle(all_tweets)\n",
    "\n",
    "all_tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['broken',\n",
       " 'luggage',\n",
       " 'mild',\n",
       " 'case',\n",
       " 'food',\n",
       " 'poisoning',\n",
       " 'ua',\n",
       " 'lhr-iah.',\n",
       " 'email',\n",
       " 'this?']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all words from all tweets\n",
    "all_words_list = [word for (document, sentiment) in all_tweets for word in document.split()]\n",
    "all_words_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 2500 most frequently appearing keywords in the corpus\n",
    "word_items = all_words.most_common(2500)\n",
    "word_features = [word for (word,count) in word_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flight', 'get', 'cancelled', 'customer', 'service', 'thanks', 'im', 'amp', 'thank', 'hold', 'still', 'us', 'hours', 'time', 'cant', 'plane', 'one', 'would', 'call', 'delayed', 'late', 'help', 'got', 'hour', 'gate', 'need', 'flights', 'back', 'like', 'flightled', 'please', 'bag', 'waiting', 'flight', 'trying', 'phone', 'never', 'ive', 'fly', 'guys', 'even', 'wait', 'going', 'airline', 'last', 'great', 'make', 'told', 'another', 'people']\n",
      "2431\n"
     ]
    }
   ],
   "source": [
    "# remove punctuations from word_features\n",
    "pattern = re.compile(r'\\W')\n",
    "word_features = [re.sub(pattern, '', word) for word in word_features]\n",
    "\n",
    "\n",
    "# remove '' from word_features\n",
    "word_features = [word for word in word_features if word != '']\n",
    "# remove single letter words from word_features\n",
    "word_features = [word for word in word_features if len(word) > 1]\n",
    "\n",
    "print(word_features[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2431\n"
     ]
    }
   ],
   "source": [
    "print(len(word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from all tweets\n",
    "bow_featureset = [(document_features(d, word_features), c) for (d,c) in all_tweets]\n",
    "X_bow = [d for (d,c) in all_tweets]\n",
    "Y_bow = [c for (d,c) in all_tweets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True, max_features=2000)\n",
    "\n",
    "model = Pipeline([('cv', cv), ('nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'able',\n",
       " 'about',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absurd',\n",
       " 'abt',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accommodate',\n",
       " 'accommodating',\n",
       " 'accommodations',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'acct',\n",
       " 'accurate',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actions',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'admirals',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'advise',\n",
       " 'advisory',\n",
       " 'afford',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'again',\n",
       " 'age',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ago',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'ahold',\n",
       " 'air',\n",
       " 'aircraft',\n",
       " 'airline',\n",
       " 'airlines',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'airways',\n",
       " 'albany',\n",
       " 'alert',\n",
       " 'all',\n",
       " 'alliance',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'alternate',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americanairlines',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'and',\n",
       " 'angry',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announcements',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'apologies',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'app']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_vector = cv.fit(X_bow)\n",
    "BOW_vector.get_feature_names()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "scores = cross_validate(model, X_bow, Y_bow, return_estimator=True,\n",
    "                        scoring = scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : \n",
      "[0.90428757 0.90294627 0.90511265 0.90901213 0.91507799]\n",
      "Precision : \n",
      "[0.85359122 0.85706053 0.85912356 0.86668804 0.87375925]\n",
      "Recall : \n",
      "[0.85192267 0.837476   0.8435596  0.84784234 0.86185689]\n",
      "F1 : \n",
      "[0.85275276 0.84670181 0.85098628 0.8567573  0.8676102 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \")\n",
    "print(scores['test_accuracy'])\n",
    "print(\"Precision : \")\n",
    "print(scores['test_precision_macro'])\n",
    "print(\"Recall : \")\n",
    "print(scores['test_recall_macro'])\n",
    "print(\"F1 : \")\n",
    "print(scores['test_f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv', CountVectorizer(binary=True, max_features=2000)),\n",
       "                ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_bow, Y_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype='<U8')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(['that was a great flight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubjectivityList "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "\n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL.keys():\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)  \n",
    "        else:\n",
    "            features['negativecount'] = 0\n",
    "            features['positivecount'] = 0    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a dictionary where you can look up words and get back \n",
    "#     the four items of subjectivity information described above\n",
    "def readSubjectivity(path):\n",
    "    flexicon = open(path, 'r')\n",
    "    # initialize an empty dictionary\n",
    "    sldict = { }\n",
    "    for line in flexicon:\n",
    "        fields = line.split()   # default is to split on whitespace\n",
    "        # split each field on the '=' and keep the second part as the value\n",
    "        strength = fields[0].split(\"=\")[1]\n",
    "        word = fields[2].split(\"=\")[1]\n",
    "        posTag = fields[3].split(\"=\")[1]\n",
    "        stemmed = fields[4].split(\"=\")[1]\n",
    "        polarity = fields[5].split(\"=\")[1]\n",
    "        if (stemmed == 'y'):\n",
    "            isStemmed = True\n",
    "        else:\n",
    "            isStemmed = False\n",
    "        # put a dictionary entry with the word as the keyword\n",
    "        #     and a list of the other values\n",
    "        sldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "    return sldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abandoned': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'abandonment': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'abandon': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'abase': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'abasement': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'abash': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'abate': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'abdicate': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'aberration': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'abhor': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'abhorred': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'abhorrence': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'abhorrent': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'abhorrently': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'abhors': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'abidance': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'abide': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'abject': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'abjectly': ['strongsubj', 'adverb', False, 'negative'],\n",
       " 'abjure': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'abilities': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'ability': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'able': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'abnormal': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'abolish': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'abominable': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'abominably': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'abominate': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'abomination': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'above': ['weaksubj', 'anypos', False, 'positive'],\n",
       " 'above-average': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'abound': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'abrade': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'abrasive': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'abrupt': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'abscond': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'absence': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'absentee': ['weaksubj', 'anypos', True, 'negative'],\n",
       " 'absent-minded': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'absolve': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'absolute': ['strongsubj', 'adj', False, 'neutral'],\n",
       " 'absolutely': ['strongsubj', 'adj', False, 'neutral'],\n",
       " 'absorbed': ['weaksubj', 'verb', False, 'neutral'],\n",
       " 'absurd': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'absurdity': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'absurdly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'absurdness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'abundant': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'abundance': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'abuse': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'abuses': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'abusive': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'abysmal': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'abysmally': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'abyss': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'accede': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'accentuate': ['strongsubj', 'verb', True, 'neutral'],\n",
       " 'accept': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'acceptance': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'acceptable': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'accessible': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'accidental': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'acclaim': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'acclaimed': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'acclamation': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'accolade': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'accolades': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'accommodative': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'accomplish': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'accomplishment': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'accomplishments': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'accord': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'accordance': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'accordantly': ['weaksubj', 'adverb', False, 'positive'],\n",
       " 'accost': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'accountable': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'accurate': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'accurately': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'accursed': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'accusation': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'accusations': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'accuse': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'accuses': ['strongsubj', 'verb', False, 'negative'],\n",
       " 'accusing': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'accusingly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'acerbate': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'acerbic': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'acerbically': ['strongsubj', 'adverb', False, 'negative'],\n",
       " 'ache': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'achievable': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'achieve': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'achievement': ['weaksubj', 'anypos', False, 'positive'],\n",
       " 'achievements': ['weaksubj', 'anypos', False, 'positive'],\n",
       " 'acknowledge': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'acknowledgement': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'acquit': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'acrid': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'acridly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'acridness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'acrimonious': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'acrimoniously': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'acrimony': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'active': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'activist': ['strongsubj', 'noun', False, 'neutral'],\n",
       " 'actual': ['strongsubj', 'anypos', False, 'neutral'],\n",
       " 'actuality': ['strongsubj', 'anypos', False, 'neutral'],\n",
       " 'actually': ['strongsubj', 'anypos', False, 'neutral'],\n",
       " 'acumen': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'adamant': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'adamantly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'adaptable': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'adaptability': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'adaptive': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'addict': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'addiction': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'adept': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'adeptly': ['weaksubj', 'adverb', False, 'positive'],\n",
       " 'adequate': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'adherence': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'adherent': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'adhesion': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'admirable': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'admirer': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'admirably': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'admiration': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'admire': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'admiring': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'admiringly': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'admission': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'admit': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'admittedly': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'admonish': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'admonisher': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'admonishingly': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'admonishment': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'admonition': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'adolescents': ['strongsubj', 'noun', False, 'neutral'],\n",
       " 'adorable': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'adore': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'adored': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'adorer': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'adoring': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'adoringly': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'adrift': ['weaksubj', 'anypos', False, 'negative'],\n",
       " 'adroit': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'adroitly': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'adulate': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'adulation': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'adulatory': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'adulterate': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'adulterated': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'adulteration': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'advanced': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'advantage': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'advantageous': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'advantages': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'adventure': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'adventuresome': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'adventurism': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'adventurous': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'adversarial': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'adversary': ['weaksubj', 'noun', True, 'negative'],\n",
       " 'adverse': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'adversity': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'advice': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'advisable': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'advocate': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'advocacy': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'affable': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'affability': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'affably': ['weaksubj', 'adverb', False, 'positive'],\n",
       " 'affect': ['weaksubj', 'verb', True, 'neutral'],\n",
       " 'affectation': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'affected': ['strongsubj', 'adj', False, 'neutral'],\n",
       " 'affection': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'affectionate': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'affinity': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'affirm': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'affirmation': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'affirmative': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'afflict': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'affliction': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'afflictive': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'affluent': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'affluence': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'afford': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'affordable': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'affront': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'afloat': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'afraid': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'against': ['weaksubj', 'anypos', False, 'negative'],\n",
       " 'aggravate': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'aggravating': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'aggravation': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'aggression': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'aggressive': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'aggressiveness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'aggressor': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'aggrieve': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'aggrieved': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'aghast': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'agile': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'agilely': ['weaksubj', 'adverb', False, 'positive'],\n",
       " 'agility': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'agitate': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'agitated': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'agitation': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'agitator': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'agonies': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'agonize': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'agonizing': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'agonizingly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'agony': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'agree': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'agreeability': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'agreeable': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'agreeableness': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'agreeably': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'agreement': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'aha': ['strongsubj', 'anypos', True, 'neutral'],\n",
       " 'ail': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'ailment': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'aimless': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'air': ['strongsubj', 'verb', True, 'neutral'],\n",
       " 'airs': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'alarm': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'alarmed': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'alarming': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'alarmingly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'alas': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'alert': ['weaksubj', 'adj', False, 'neutral'],\n",
       " 'alienate': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'alienated': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'alienation': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'all-time': ['weaksubj', 'noun', False, 'neutral'],\n",
       " 'allay': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'allegation': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'allegations': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'allege': ['weaksubj', 'verb', False, 'negative'],\n",
       " 'allegorize': ['strongsubj', 'verb', True, 'neutral'],\n",
       " 'allergic': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'alleviate': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'alliance': ['strongsubj', 'anypos', False, 'neutral'],\n",
       " 'alliances': ['strongsubj', 'noun', False, 'neutral'],\n",
       " 'allow': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'allowable': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'allure': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'alluring': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'alluringly': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'allusion': ['strongsubj', 'noun', False, 'neutral'],\n",
       " 'allusions': ['strongsubj', 'anypos', False, 'neutral'],\n",
       " 'ally': ['strongsubj', 'noun', True, 'positive'],\n",
       " 'almighty': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'aloof': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'altercation': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'although': ['weaksubj', 'anypos', True, 'negative'],\n",
       " 'altogether': ['strongsubj', 'anypos', False, 'neutral'],\n",
       " 'altruist': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'altruistic': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'altruistically': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'amaze': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'amazed': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'amazement': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'amazing': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'amazingly': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'ambiguous': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'ambiguity': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'ambitious': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'ambitiously': ['weaksubj', 'anypos', True, 'positive'],\n",
       " 'ambivalence': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'ambivalent': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'ambush': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'ameliorate': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'amenable': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'amenity': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'amiability': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'amiabily': ['strongsubj', 'adverb', False, 'positive'],\n",
       " 'amiable': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'amicability': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'amicable': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'amicably': ['strongsubj', 'adverb', False, 'positive'],\n",
       " 'amiss': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'amity': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'amnesty': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'amour': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'ample': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'amply': ['weaksubj', 'adverb', False, 'positive'],\n",
       " 'amplify': ['weaksubj', 'verb', True, 'neutral'],\n",
       " 'amputate': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'amuse': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'amusement': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'amusing': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'amusingly': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'analytical': ['weaksubj', 'adj', False, 'neutral'],\n",
       " 'anarchism': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anarchist': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anarchistic': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'anarchy': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anemic': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'angel': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'angelic': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'anger': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'angrily': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'angriness': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'angry': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'anguish': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'annihilate': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'annihilation': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'animated': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'animosity': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'annoy': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'annoyance': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'annoyed': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'annoying': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'annoyingly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'anomalous': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'anomaly': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'antagonism': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'antagonist': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'antagonistic': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'antagonize': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'anti-': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anti-American': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anti-Israeli': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anti-Semites': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anti-US': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anti-occupation': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anti-proliferation': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anti-social': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'anti-white': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'antipathy': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'antiquated': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'antithetical': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anxieties': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anxiety': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anxious': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'anxiously': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'anxiousness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'anyhow': ['strongsubj', 'anypos', True, 'neutral'],\n",
       " 'anyway': ['strongsubj', 'anypos', True, 'neutral'],\n",
       " 'anyways': ['strongsubj', 'anypos', True, 'neutral'],\n",
       " 'apathetic': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'apathetically': ['strongsubj', 'adverb', False, 'negative'],\n",
       " 'apathy': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'ape': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'apocalypse': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'apocalyptic': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'apologist': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'apologists': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'apostle': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'apotheosis': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'appal': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'appall': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'appalled': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'appalling': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'appallingly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'apparent': ['weaksubj', 'anypos', True, 'neutral'],\n",
       " 'apparently': ['weaksubj', 'anypos', True, 'neutral'],\n",
       " 'appeal': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'appealing': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'appear': ['weaksubj', 'anypos', True, 'neutral'],\n",
       " 'appearance': ['weaksubj', 'noun', False, 'neutral'],\n",
       " 'appease': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'applaud': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'appreciable': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'appreciate': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'appreciation': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'appreciative': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'appreciatively': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'appreciativeness': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'apprehend': ['weaksubj', 'verb', True, 'neutral'],\n",
       " 'apprehension': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'apprehensions': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'apprehensive': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'apprehensively': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'appropriate': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'approval': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'approve': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'apt': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'aptly': ['weaksubj', 'adverb', False, 'positive'],\n",
       " 'aptitude': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'arbitrary': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'arcane': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'archaic': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'ardent': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'ardently': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'ardor': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'arduous': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'arduously': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'aristocratic': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'argue': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'argument': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'argumentative': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'arguments': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'arousal': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'arouse': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'arousing': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'arresting': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'arrogance': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'arrogant': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'arrogantly': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'articulate': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'artificial': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'ascendant': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'ascertainable': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'ashamed': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'asinine': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'asininely': ['strongsubj', 'adverb', False, 'negative'],\n",
       " 'asinininity': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'askance': ['strongsubj', 'adverb', False, 'negative'],\n",
       " 'asperse': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'aspersion': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'aspersions': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'aspiration': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'aspirations': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'aspire': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'assail': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'assassinate': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'assassin': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'assault': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'assent': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'assertions': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'assertive': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'assess': ['strongsubj', 'verb', True, 'neutral'],\n",
       " 'assessment': ['weaksubj', 'noun', False, 'neutral'],\n",
       " 'assessments': ['weaksubj', 'noun', False, 'neutral'],\n",
       " 'asset': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'assiduous': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'assiduously': ['strongsubj', 'adverb', False, 'positive'],\n",
       " 'assuage': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'assumption': ['weaksubj', 'noun', False, 'neutral'],\n",
       " 'assurance': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'assurances': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'assure': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'assuredly': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'astonish': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'astonished': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'astonishing': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'astonishingly': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'astonishment': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'astound': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'astounded': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'astounding': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'astoundingly': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'astray': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'astronomic': ['strongsubj', 'adj', False, 'neutral'],\n",
       " 'astronomical': ['strongsubj', 'adj', False, 'neutral'],\n",
       " 'astronomically': ['strongsubj', 'anypos', False, 'neutral'],\n",
       " 'astute': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'astutely': ['strongsubj', 'adverb', False, 'positive'],\n",
       " 'asunder': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'asylum': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'atrocious': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'atrocities': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'atrocity': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'atrophy': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'attack': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'attain': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'attainable': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'attentive': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'attest': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'attitude': ['strongsubj', 'noun', False, 'neutral'],\n",
       " 'attitudes': ['weaksubj', 'noun', False, 'neutral'],\n",
       " 'attraction': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'attractive': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'attractively': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'attune': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'audacious': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'audaciously': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'audaciousness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'audacity': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'auspicious': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'austere': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'authentic': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'authoritarian': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'authoritative': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'autocrat': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'autocratic': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'award': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'awareness': ['strongsubj', 'noun', False, 'neutral'],\n",
       " 'aware': ['strongsubj', 'adj', False, 'neutral'],\n",
       " 'autonomous': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'avalanche': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'avarice': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'avaricious': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'avariciously': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'avenge': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'aver': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'averse': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'aversion': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'avid': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'avidly': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'avoid': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'avoidance': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'awe': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'awed': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'awesome': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'awesomely': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'awesomeness': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'awestruck': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'awful': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'awfully': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'awfulness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'awkward': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'awkwardness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'ax': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'babble': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'baby': ['strongsubj', 'noun', False, 'neutral'],\n",
       " 'back': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'backbite': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'backbiting': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'backbone': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'backward': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'backwardness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bad': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'badly': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'baffle': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'baffled': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'bafflement': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'baffling': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'bait': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'balanced': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'balk': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'banal': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'banalize': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bane': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'banish': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'banishment': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bankrupt': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'bar': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'barbarian': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'barbaric': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'barbarically': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'barbarity': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'barbarous': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'barbarously': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'barely': ['weaksubj', 'anypos', True, 'negative'],\n",
       " 'bargain': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'barren': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'baseless': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'bashful': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'basic': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'basically': ['weaksubj', 'anypos', False, 'neutral'],\n",
       " 'bask': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'bastard': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'batons': ['weaksubj', 'noun', False, 'neutral'],\n",
       " 'battered': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'battering': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'battle': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'battle-lines': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'battlefield': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'battleground': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'batty': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'beacon': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'bearish': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'beast': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'beastly': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'beatify': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'beauteous': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'beautiful': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'beautifully': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'beautify': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'beauty': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'bedlam': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bedlamite': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'befit': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'befitting': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'befoul': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'befriend': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'beg': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'beggar': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'beggarly': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'begging': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'beguile': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'belated': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'belabor': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'beleaguer': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'belie': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'belief': ['strongsubj', 'noun', False, 'neutral'],\n",
       " 'beliefs': ['weaksubj', 'noun', False, 'neutral'],\n",
       " 'believable': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'believe': ['strongsubj', 'verb', True, 'neutral'],\n",
       " 'belittle': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'belittled': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'belittling': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'bellicose': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'belligerence': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'belligerent': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'belligerently': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'beloved': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'bemoan': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bemoaning': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'bemused': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'benefactor': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'beneficial': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'beneficent': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'beneficially': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'beneficiary': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'benefit': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'benefits': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'benevolence': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'benevolent': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'benign': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'bent': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'berate': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bereave': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bereavement': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bereft': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'berserk': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'beseech': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'beset': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'besides': ['strongsubj', 'anypos', True, 'neutral'],\n",
       " 'besiege': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'besmirch': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'best': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'best-known': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'best-performing': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'best-selling': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'bestial': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'betray': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'betrayal': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'betrayals': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'betrayer': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'better': ['weaksubj', 'anypos', True, 'positive'],\n",
       " 'better-known': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'better-than-expected': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'bewail': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'beware': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'bewilder': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bewildered': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'bewildering': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'bewilderingly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'bewilderment': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bewitch': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bias': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'biased': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'biases': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bicker': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bickering': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'bid-rigging': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'big': ['weaksubj', 'adj', False, 'neutral'],\n",
       " 'bitch': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bitchy': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'biting': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'bitingly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'bitter': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'bitterly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'bitterness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bizarre': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'blab': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'blabber': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'black': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'blackmail': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'blah': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'blame': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'blameless': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'blameworthy': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'bland': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'blandish': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'blaspheme': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'blasphemous': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'blasphemy': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'blast': ['weaksubj', 'anypos', True, 'negative'],\n",
       " 'blasted': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'blatant': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'blatantly': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'blather': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bleak': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'bleakly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'bleakness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bleed': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'blemish': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'bless': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'blessing': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'blind': ['weaksubj', 'anypos', False, 'negative'],\n",
       " 'blinding': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'blindingly': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'blindness': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'blindside': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bliss': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'blissful': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'blissfully': ['strongsubj', 'adverb', False, 'positive'],\n",
       " 'blister': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'blistering': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'blithe': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'bloated': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'block': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'blockhead': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'blood': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'bloodshed': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bloodthirsty': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'bloody': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'bloom': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'blossom': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'blow': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'blunder': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'blundering': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'blunders': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'blunt': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'blur': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'blurt': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'boast': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'boastful': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'boggle': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bogus': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'boil': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'boiling': ['strongsubj', 'adverb', False, 'negative'],\n",
       " 'boisterous': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'bold': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'boldly': ['strongsubj', 'adverb', False, 'positive'],\n",
       " 'boldness': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'bolster': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'bombard': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'bomb': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'bombardment': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bombastic': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'bondage': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bonkers': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bonny': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'bonus': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'boom': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'booming': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'boost': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'bore': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'boredom': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'boring': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'botch': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bother': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bothersome': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'boundless': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'bountiful': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'bowdlerize': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'boycott': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'brag': ['strongsubj', 'verb', True, 'both'],\n",
       " 'braggart': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bragger': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'brains': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'brainwash': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'brainy': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'brash': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'brashly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'brashness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'brat': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bravado': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'brave': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'bravery': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'brazen': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'brazenly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'brazenness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'breach': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'break': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'break-point': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'breakdown': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'breakthrough': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'breakthroughs': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'breathlessness': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'breathtaking': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'breathtakingly': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'bright': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'brighten': ['weaksubj', 'anypos', True, 'positive'],\n",
       " 'brightness': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'brilliance': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'brilliant': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'brilliantly': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'brimstone': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'brisk': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'bristle': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'brittle': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'broad': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'broad-based': ['weaksubj', 'adj', False, 'neutral'],\n",
       " 'broke': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'broken-hearted': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'brood': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'brook': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'brotherly': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'browbeat': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bruise': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'brusque': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'brutal': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'brutalising': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'brutalities': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'brutality': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'brutalize': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'brutalizing': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'brutally': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'brute': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'brutish': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'bug': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'buckle': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'bulky': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'bull': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'bullies': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bullish': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'bully': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'bullyingly': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'bum': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'bumpy': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'bungle': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'bungler': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'bunk': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'buoyant': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'burden': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'burdensome': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'burdensomely': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'burn': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'busy': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'busybody': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'butcher': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'butchery': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'byzantine': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'cackle': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'cajole': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'calamities': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'calamitous': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'calamitously': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'calamity': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'callous': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'calm': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'calming': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'calmness': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'calumniate': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'calumniation': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'calumnies': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'calumnious': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'calumniously': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'calumny': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'cancer': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'cancerous': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'candid': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'candor': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'cannibal': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'cannibalize': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'capable': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'capability': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'capably': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'capitalize': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'capitulate': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'capricious': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'capriciously': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'capriciousness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'capsize': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'captivate': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'captivating': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'captivation': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'captive': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'care': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'carefree': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'careful': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'careless': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'carelessness': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'caricature': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'carnage': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'carp': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'cartoon': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'cartoonish': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'cash-strapped': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'castigate': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'casualty': ['weaksubj', 'noun', True, 'negative'],\n",
       " 'cataclysm': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'cataclysmal': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'cataclysmic': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'cataclysmically': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'catalyst': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'catastrophe': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'catastrophes': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'catastrophic': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'catastrophically': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'catchy': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'caustic': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'caustically': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'cautionary': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'cautious': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'cave': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'ceaseless': ['weaksubj', 'adj', False, 'neutral'],\n",
       " 'celebrate': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'celebrated': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'celebration': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'celebratory': ['strongsubj', 'anypos', True, 'positive'],\n",
       " 'celebrity': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'censure': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'central': ['weaksubj', 'adj', False, 'neutral'],\n",
       " 'certain': ['weaksubj', 'anypos', True, 'neutral'],\n",
       " 'certainly': ['strongsubj', 'anypos', False, 'neutral'],\n",
       " 'certified': ['weaksubj', 'adj', False, 'neutral'],\n",
       " 'chafe': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'chaff': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'chagrin': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'challenge': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'challenging': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'champion': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'champ': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'chant': ['weaksubj', 'noun', False, 'neutral'],\n",
       " 'chaos': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'chaotic': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'charisma': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'charismatic': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'charitable': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'charity': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'charm': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'charming': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'charmingly': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'chaste': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'chasten': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'chastise': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'chastisement': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'chatter': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'chatterbox': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'cheapen': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'cheap': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'cheat': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'cheater': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'cheer': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'cheery': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'cheerful': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'cheerless': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'cherish': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'cherished': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'cherub': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'chic': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'chide': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'childish': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'chill': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'chilly': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'chit': ['strongsubj', 'anypos', True, 'negative'],\n",
       " 'chivalry': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'chivalrous': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'choppy': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'choke': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'chore': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'chronic': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'chum': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'civility': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'civilization': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'civilize': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'civil': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'claim': ['weaksubj', 'anypos', True, 'neutral'],\n",
       " 'clamor': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'clamorous': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'clandestine': ['weaksubj', 'adj', False, 'neutral'],\n",
       " 'clarity': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'clash': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'classic': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'clean': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'cleanliness': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'cleanse': ['weaksubj', 'verb', False, 'positive'],\n",
       " 'clear': ['weaksubj', 'anypos', True, 'positive'],\n",
       " 'clear-cut': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'clearer': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'clearly': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'clever': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'cliche': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'cliched': ['strongsubj', 'anypos', False, 'negative'],\n",
       " 'clique': ['weaksubj', 'noun', False, 'negative'],\n",
       " 'clog': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'close': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'closeness': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'cloud': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'clout': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'clumsy': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'co-operation': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'coarse': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'coax': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'cocky': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'coddle': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'coerce': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'coercion': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'coercive': ['strongsubj', 'adj', False, 'negative'],\n",
       " 'cogent': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'cohesive': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'cogitate': ['strongsubj', 'verb', True, 'neutral'],\n",
       " 'cognizance': ['weaksubj', 'noun', False, 'neutral'],\n",
       " 'cognizant': ['strongsubj', 'noun', False, 'neutral'],\n",
       " 'cohere': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'coherence': ['strongsubj', 'noun', False, 'positive'],\n",
       " 'coherent': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'cohesion': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'cold': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'coldly': ['strongsubj', 'adverb', False, 'negative'],\n",
       " 'collapse': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'collide': ['weaksubj', 'verb', True, 'negative'],\n",
       " 'collude': ['strongsubj', 'verb', True, 'negative'],\n",
       " 'collusion': ['strongsubj', 'noun', False, 'negative'],\n",
       " 'colorful': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'colossal': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'combative': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'comeback': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'comedy': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'comely': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'comfort': ['weaksubj', 'verb', True, 'positive'],\n",
       " 'comfortable': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'comfortably': ['weaksubj', 'adverb', False, 'positive'],\n",
       " 'comforting': ['strongsubj', 'anypos', False, 'positive'],\n",
       " 'comical': ['weaksubj', 'adj', False, 'negative'],\n",
       " 'commend': ['strongsubj', 'verb', True, 'positive'],\n",
       " 'commendable': ['strongsubj', 'adj', False, 'positive'],\n",
       " 'commendably': ['strongsubj', 'adverb', False, 'positive'],\n",
       " 'commensurate': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'commonsense': ['weaksubj', 'noun', False, 'positive'],\n",
       " 'commonsensible': ['weaksubj', 'adj', False, 'positive'],\n",
       " 'commonsensibly': ['weaksubj', 'adverb', False, 'positive'],\n",
       " ...}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a path to where the subjectivity file resides on your disk\n",
    "SLpath = \"subjclueslen1-HLTEMNLP05.tff\"\n",
    "\n",
    "# copy the readSubjectivity function into your session and cal the fn\n",
    "SL = readSubjectivity(SLpath)\n",
    "SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'broken luggage mild case food poisoning ua lhr-iah. email this?'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SL_featuresets = [(SL_features(d.split(\" \"), cv.get_feature_names(), SL), c) for (d, c) in all_tweets]\n",
    "X_SL = [[v['negativecount'], v['positivecount']] for (v,k) in SL_featuresets]\n",
    "Y_SL = [k for (v,k) in SL_featuresets]\n",
    "X_SL[:7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_SL[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : \n",
      "[0.7955825  0.79462738 0.79332756 0.79462738 0.79506066]\n",
      "Precision : \n",
      "[0.65663052 0.64180551 0.62708028 0.64399066 0.65026478]\n",
      "Recall : \n",
      "[0.51596736 0.5199171  0.51752613 0.51541958 0.51647666]\n",
      "F1 : \n",
      "[0.48158564 0.49148345 0.48741558 0.48113381 0.48310784]\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "scores = cross_validate(model, X_SL, Y_SL, return_estimator=True,\n",
    "                        scoring = scoring)\n",
    "print(\"Accuracy : \")\n",
    "print(scores['test_accuracy'])\n",
    "print(\"Precision : \")\n",
    "print(scores['test_precision_macro'])\n",
    "print(\"Recall : \")\n",
    "print(scores['test_recall_macro'])\n",
    "print(\"F1 : \")\n",
    "print(scores['test_f1_macro'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7962521663778163"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = SL_featuresets[:int(len(SL_featuresets)*0.8):], SL_featuresets[:int(len(SL_featuresets)*0.8)]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           positivecount = 6              positi : negati =      6.3 : 1.0\n",
      "           positivecount = 5              positi : negati =      5.5 : 1.0\n",
      "           positivecount = 7              positi : negati =      3.9 : 1.0\n",
      "           positivecount = 8              positi : negati =      3.9 : 1.0\n",
      "           negativecount = 4              negati : positi =      3.8 : 1.0\n",
      "           negativecount = 3              negati : positi =      3.5 : 1.0\n",
      "           positivecount = 4              positi : negati =      2.9 : 1.0\n",
      "           positivecount = 2              positi : negati =      2.3 : 1.0\n",
      "           negativecount = 2              negati : positi =      2.0 : 1.0\n",
      "           positivecount = 1              negati : positi =      1.9 : 1.0\n",
      "           positivecount = 3              positi : negati =      1.9 : 1.0\n",
      "           negativecount = 1              negati : positi =      1.8 : 1.0\n",
      "           positivecount = 0              negati : positi =      1.1 : 1.0\n",
      "           negativecount = 0              positi : negati =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negation Word feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8648180242634316"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this list of negation words \n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "NOT_featuresets = [(NOT_features(d.split(\" \"), cv.get_feature_names(), negationwords), c) for (d, c) in all_tweets]\n",
    "\n",
    "\n",
    "train_set, test_set = NOT_featuresets[int(len(NOT_featuresets)*0.8):], NOT_featuresets[:int(len(NOT_featuresets)*0.8)]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 V_thank = True           positi : negati =     25.3 : 1.0\n",
      "                  V_you! = False          positi : negati =     22.9 : 1.0\n",
      "               V_awesome = True           positi : negati =     22.0 : 1.0\n",
      "               V_forward = True           positi : negati =     22.0 : 1.0\n",
      "               V_amazing = True           positi : negati =     19.4 : 1.0\n",
      "                 V_kudos = True           positi : negati =     16.8 : 1.0\n",
      "                    V_:) = False          positi : negati =     14.5 : 1.0\n",
      "                 V_loved = True           positi : negati =     14.2 : 1.0\n",
      "               V_thanks! = False          positi : negati =     13.8 : 1.0\n",
      "                  V_love = True           positi : negati =     12.3 : 1.0\n",
      "                   V_win = True           positi : negati =     11.7 : 1.0\n",
      "              V_service! = False          positi : negati =     11.7 : 1.0\n",
      "               V_delayed = True           negati : positi =     11.1 : 1.0\n",
      "                 V_great = True           positi : negati =     10.2 : 1.0\n",
      "                 V_hours = True           negati : positi =     10.1 : 1.0\n",
      "               V_thanks, = False          positi : negati =     10.1 : 1.0\n",
      "                  V_best = True           positi : negati =      9.6 : 1.0\n",
      "                    V_;) = False          positi : negati =      9.1 : 1.0\n",
      "               V_NOTwait = True           positi : negati =      9.1 : 1.0\n",
      "                   V_aa! = False          positi : negati =      9.1 : 1.0\n",
      "                  V_cool = True           positi : negati =      9.1 : 1.0\n",
      "                  V_day! = False          positi : negati =      9.1 : 1.0\n",
      "              V_deserves = True           positi : negati =      9.1 : 1.0\n",
      "                  V_set. = False          positi : negati =      9.1 : 1.0\n",
      "                  V_soon = True           positi : negati =      9.1 : 1.0\n",
      "              V_awesome. = False          positi : negati =      8.5 : 1.0\n",
      "                V_helped = True           positi : negati =      8.5 : 1.0\n",
      "             V_flightled = True           negati : positi =      8.1 : 1.0\n",
      "                V_thanks = True           positi : negati =      7.7 : 1.0\n",
      "               V_despite = True           positi : negati =      7.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11541,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-106-9934b6f9424e>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_Not = np.array([np.array(list(v.values())) for (v,k) in NOT_featuresets])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([array([False, False, False, ..., False, False, False]),\n",
       "       array([False, False, False, ..., False, False, False]),\n",
       "       array([False, False, False, ..., False, False, False]),\n",
       "       array([False, False, False, ..., False, False, False]),\n",
       "       array([False, False, False, ..., False, False, False]),\n",
       "       array([False, False, False, ..., False, False, False]),\n",
       "       array([False, False, False, ..., False, False, False]),\n",
       "       array([ True, False, False, ..., False, False, False]),\n",
       "       array([False, False, False, ..., False, False, False]),\n",
       "       array([False, False, False, ..., False, False, False])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Not = np.array([np.array(list(v.values())) for (v,k) in NOT_featuresets])\n",
    "Y_Not = [k for (v,k) in NOT_featuresets]\n",
    "print(X_Not.shape)\n",
    "X_Not[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round  1  accuracy:  0.8884845334026514\n",
      "Precision:  0.8884845334026514\n",
      "Recall:  0.8884845334026514\n",
      "F1:  0.8884845334026514\n",
      "\n",
      "\n",
      "Round  2  accuracy:  0.8908240187158825\n",
      "Precision:  0.8908240187158825\n",
      "Recall:  0.8908240187158825\n",
      "F1:  0.8908240187158825\n",
      "\n",
      "\n",
      "Round  3  accuracy:  0.8996620743436444\n",
      "Precision:  0.8996620743436444\n",
      "Recall:  0.8996620743436444\n",
      "F1:  0.8996620743436444\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_folds = 3\n",
    "subset_size = int(len(NOT_featuresets)/num_folds)\n",
    "avg_f1 = 0\n",
    "for i in range(num_folds):\n",
    "    testing_this_round = NOT_featuresets[i*subset_size:(i+1)*subset_size]\n",
    "    training_this_round = NOT_featuresets[:i*subset_size] + NOT_featuresets[(i+1)*subset_size:]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_this_round)\n",
    "    y_pred = classifier.classify_many([v for (v,k) in testing_this_round])\n",
    "    y_actual = [k for (v,k) in testing_this_round]\n",
    "    print(\"Round \", i+1, \" accuracy: \", nltk.classify.accuracy(classifier, testing_this_round))\n",
    "    cm = nltk.ConfusionMatrix(y_pred, y_actual)\n",
    "    # compute precision and recall from the confusion matrix\n",
    "    labels = ['negative', 'positive']\n",
    "    truePos = 0\n",
    "    falsePos = 0\n",
    "    falseNeg = 0\n",
    "\n",
    "    for i in labels:\n",
    "        for j in labels:\n",
    "            if i==j:\n",
    "                truePos += cm[i,j]\n",
    "            else:\n",
    "                falseNeg += cm[i,j]\n",
    "                falsePos += cm[j,i]\n",
    "    print(\"Precision: \", truePos/(truePos+falsePos))\n",
    "    print(\"Recall: \", truePos/(truePos+falseNeg))\n",
    "    print(\"F1: \", 2*truePos/(2*truePos+falsePos+falseNeg))\n",
    "    print(\"\\n\")\n",
    "    avg_f1 += 2*truePos/(2*truePos+falsePos+falseNeg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1:  0.8929902088207261\n"
     ]
    }
   ],
   "source": [
    "print(\"Average F1: \", avg_f1/num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>organizations</th>\n",
       "      <th>uuid</th>\n",
       "      <th>thread</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>ord_in_thread</th>\n",
       "      <th>title</th>\n",
       "      <th>locations</th>\n",
       "      <th>entities</th>\n",
       "      <th>highlightText</th>\n",
       "      <th>language</th>\n",
       "      <th>persons</th>\n",
       "      <th>text</th>\n",
       "      <th>external_links</th>\n",
       "      <th>published</th>\n",
       "      <th>crawled</th>\n",
       "      <th>highlightTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>0ad32fb9226b172b960cba62027cd8be7d9dfa22</td>\n",
       "      <td>{'social': {'gplus': {'shares': 0}, 'pinterest...</td>\n",
       "      <td>Udayavani</td>\n",
       "      <td>https://m.dailyhunt.in/news/india/english/uday...</td>\n",
       "      <td>0</td>\n",
       "      <td>Karnataka: Helplines, isolation wards set up f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'persons': [], 'locations': [{'name': 'karnat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>[]</td>\n",
       "      <td>Bengaluru: Isolation wards in hospitals across...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-01-31T11:46:00.000+02:00</td>\n",
       "      <td>2020-01-31T17:18:04.007+02:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>8c3c1e13471a5e6ef7e115588ab27f308285cf2e</td>\n",
       "      <td>{'social': {'gplus': {'shares': 0}, 'pinterest...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://cnnphilippines.com/videos/2020/1/28/Hea...</td>\n",
       "      <td>0</td>\n",
       "      <td>Health dept. monitoring 24 people for possible...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'persons': [], 'locations': [{'name': 'hubei'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>[]</td>\n",
       "      <td>The government making sure that the new corona...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-01-28T02:00:00.000+02:00</td>\n",
       "      <td>2020-01-31T06:38:31.000+02:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>6333601dda63a855f38435ff80ccb89b8ed8c76a</td>\n",
       "      <td>{'social': {'gplus': {'shares': 0}, 'pinterest...</td>\n",
       "      <td>jmccorm</td>\n",
       "      <td>https://news.ycombinator.com/item?id=22194798#...</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'persons': [], 'locations': [], 'organization...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>[]</td>\n",
       "      <td>Apart from more people falling sick (as bad as...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-01-31T06:38:00.000+02:00</td>\n",
       "      <td>2020-01-31T06:39:09.006+02:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>4d639ca00ddb638ae57da43712aa50e0e56df2be</td>\n",
       "      <td>{'social': {'gplus': {'shares': 0}, 'pinterest...</td>\n",
       "      <td>rttnews.com</td>\n",
       "      <td>https://www.rttnews.com/3064615/asian-markets-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Asian Markets Mostly Higher</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'persons': [{'name': 'santos', 'sentiment': '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>[]</td>\n",
       "      <td>Asian stock markets are mostly higher on Frida...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-01-31T05:53:00.000+02:00</td>\n",
       "      <td>2020-01-31T20:00:27.007+02:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>624facc68c564b015fcab9a88ed7aaf8ea1e50da</td>\n",
       "      <td>{'social': {'gplus': {'shares': 0}, 'pinterest...</td>\n",
       "      <td>Joe Easton</td>\n",
       "      <td>http://www.bnnbloomberg.ca/tesla-soars-as-bear...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tesla soars as bearish analysts left with litt...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'persons': [{'name': 'tesla', 'sentiment': 'n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>[]</td>\n",
       "      <td>Cash flow was also “very strong,” at more than...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-01-30T02:00:00.000+02:00</td>\n",
       "      <td>2020-01-31T02:55:14.004+02:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  organizations                                      uuid  \\\n",
       "0            []  0ad32fb9226b172b960cba62027cd8be7d9dfa22   \n",
       "1            []  8c3c1e13471a5e6ef7e115588ab27f308285cf2e   \n",
       "2            []  6333601dda63a855f38435ff80ccb89b8ed8c76a   \n",
       "3            []  4d639ca00ddb638ae57da43712aa50e0e56df2be   \n",
       "4            []  624facc68c564b015fcab9a88ed7aaf8ea1e50da   \n",
       "\n",
       "                                              thread       author  \\\n",
       "0  {'social': {'gplus': {'shares': 0}, 'pinterest...    Udayavani   \n",
       "1  {'social': {'gplus': {'shares': 0}, 'pinterest...          NaN   \n",
       "2  {'social': {'gplus': {'shares': 0}, 'pinterest...      jmccorm   \n",
       "3  {'social': {'gplus': {'shares': 0}, 'pinterest...  rttnews.com   \n",
       "4  {'social': {'gplus': {'shares': 0}, 'pinterest...   Joe Easton   \n",
       "\n",
       "                                                 url  ord_in_thread  \\\n",
       "0  https://m.dailyhunt.in/news/india/english/uday...              0   \n",
       "1  http://cnnphilippines.com/videos/2020/1/28/Hea...              0   \n",
       "2  https://news.ycombinator.com/item?id=22194798#...             88   \n",
       "3  https://www.rttnews.com/3064615/asian-markets-...              0   \n",
       "4  http://www.bnnbloomberg.ca/tesla-soars-as-bear...              0   \n",
       "\n",
       "                                               title locations  \\\n",
       "0  Karnataka: Helplines, isolation wards set up f...        []   \n",
       "1  Health dept. monitoring 24 people for possible...        []   \n",
       "2                                                NaN        []   \n",
       "3                        Asian Markets Mostly Higher        []   \n",
       "4  Tesla soars as bearish analysts left with litt...        []   \n",
       "\n",
       "                                            entities  highlightText language  \\\n",
       "0  {'persons': [], 'locations': [{'name': 'karnat...            NaN  english   \n",
       "1  {'persons': [], 'locations': [{'name': 'hubei'...            NaN  english   \n",
       "2  {'persons': [], 'locations': [], 'organization...            NaN  english   \n",
       "3  {'persons': [{'name': 'santos', 'sentiment': '...            NaN  english   \n",
       "4  {'persons': [{'name': 'tesla', 'sentiment': 'n...            NaN  english   \n",
       "\n",
       "  persons                                               text external_links  \\\n",
       "0      []  Bengaluru: Isolation wards in hospitals across...             []   \n",
       "1      []  The government making sure that the new corona...             []   \n",
       "2      []  Apart from more people falling sick (as bad as...             []   \n",
       "3      []  Asian stock markets are mostly higher on Frida...             []   \n",
       "4      []  Cash flow was also “very strong,” at more than...             []   \n",
       "\n",
       "                       published                        crawled  \\\n",
       "0  2020-01-31T11:46:00.000+02:00  2020-01-31T17:18:04.007+02:00   \n",
       "1  2020-01-28T02:00:00.000+02:00  2020-01-31T06:38:31.000+02:00   \n",
       "2  2020-01-31T06:38:00.000+02:00  2020-01-31T06:39:09.006+02:00   \n",
       "3  2020-01-31T05:53:00.000+02:00  2020-01-31T20:00:27.007+02:00   \n",
       "4  2020-01-30T02:00:00.000+02:00  2020-01-31T02:55:14.004+02:00   \n",
       "\n",
       "   highlightTitle  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('mycovid.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_bow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-a694564801f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_bow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_bow' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(x_bow,y_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This MultinomialNB instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-9c6fef5f8a20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4136\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4138\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-115-9c6fef5f8a20>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This MultinomialNB instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "df['Sentiment'] = df['text'].apply(lambda x: model.predict([x])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment_Not'] = df['text'].apply(lambda x: classifier.classify(NOT_features(x.split(\" \"), cv.get_feature_names(), negationwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print complete sentences of positive tweets\n",
    "for index, row in df_pos.iterrows():\n",
    "    if index>100:\n",
    "        break\n",
    "    print(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_neg.iterrows():\n",
    "    if index>100:\n",
    "        break\n",
    "    print(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 50 adjectives in positive tweets\n",
    "pos_doc = [d.split(\" \") for d in df_pos['text']]\n",
    "adj_words = []\n",
    "for doc in pos_doc:\n",
    "    for word in doc:\n",
    "        pos = wn.synsets(word, pos='a')\n",
    "        if len(pos) > 0:\n",
    "            adj_words.append(word)\n",
    "\n",
    "pos_adjectives = pd.Series(adj_words).value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(', '.join([\"'{}'\".format(word) for word in pos_adjectives.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 50 adjectives in negative tweets\n",
    "neg_doc = [d.split(\" \") for d in df_neg['text']]\n",
    "adj_words = []\n",
    "for doc in neg_doc:\n",
    "    for word in doc:\n",
    "        pos = wn.synsets(word, pos='a')\n",
    "        if len(pos) > 0:\n",
    "            adj_words.append(word)\n",
    "\n",
    "neg_adjectives = pd.Series(adj_words).value_counts()[:50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(', '.join([\"'{}'\".format(word) for word in neg_adjectives.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
